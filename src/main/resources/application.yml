
server:
  port: 8080

logging:
  level:
    io.modelcontextprotocol:
      client: DEBUG
      spec: INFO

spring:
  ai:
    # ---- GEMINI (OpenAI-compatible endpoint) ----
    openai:
      api-key: 
      base-url: https://generativelanguage.googleapis.com/v1beta/openai
      chat:
        completions-path: /chat/completions
        options:
          model: gemini-1.5-flash
          tool-choice: auto

    # ---- OLLAMA (local) ----
    ollama:
      base-url: http://localhost:11434
      init:
        pull-model-strategy: when_missing
      chat:
        options:
          model: "qwen2.5-coder:3b"
          temperature: 0.2
          format: "json"

    # ---- MCP (SSE) ----
    mcp:
      client:
        toolcallback:
          enabled: true
        sse:
          connections:
            my-mcp-server:
              url: http://localhost:8081
